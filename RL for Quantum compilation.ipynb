{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import math\n",
    "import random\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import qutip as qt\n",
    "from qutip import sigmax, sigmay, sigmaz\n",
    "from IPython import display\n",
    "from collections import deque\n",
    "from typing import Optional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test our DQN, we will be using the set of single-qubit quantum gates below for the compilation. This is a set of universal quantum gates, so they can approximate any $2 \\times 2$  unitary matrix (single-qubit gate) with arbitraty precision. The goal being that our algorithm finds the optimal way to compile (as in approximate) a single-qubit gate with a determined precision.\n",
    "\n",
    "$V_1 = \\frac{1}{\\sqrt{5}} \\begin{pmatrix}\n",
    "1 & 2i \\\\\n",
    "2i & 1\n",
    "\\end{pmatrix}, \\quad\n",
    "V_2 = \\frac{1}{\\sqrt{5}} \\begin{pmatrix}\n",
    "1 & 2 \\\\\n",
    "-2 & 1\n",
    "\\end{pmatrix}, \\quad\n",
    "V_3 = \\frac{1}{\\sqrt{5}} \\begin{pmatrix}\n",
    "1 + 2i & 0 \\\\\n",
    "0 & 1 - 2i\n",
    "\\end{pmatrix}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat1 = (1/np.sqrt(5)) * np.array([[1, 2j], [2j, 1]])\n",
    "mat2 = (1/np.sqrt(5)) * np.array([[1, 2], [-2, 1]])\n",
    "mat3 = (1/np.sqrt(5)) * np.array([[1 + 2j, 0], [0, 1 - 2j]])\n",
    "dim = [[2],[2]]\n",
    "efficient_set = []\n",
    "\n",
    "v1 = qt.Qobj(mat1, dims = dim)\n",
    "v2 = qt.Qobj(mat2, dims = dim)\n",
    "v3 = qt.Qobj(mat3, dims = dim)\n",
    "\n",
    "efficient_set = [v1, v2, v3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the training process, the target gate will be a product of randomly sampled matrices from a set constituted from the matrices above (which should simplify the training process), as well as fundamental gates in quantum computing like the Pauli gates, and the Hadamard gates. We implement the function that will do that :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = qt.gates.hadamard_transform()\n",
    "gateList = [h, sigmax(), sigmay(), sigmaz(), v1, v2, v3]\n",
    "\n",
    "def shuffling(lst, max_length) :\n",
    "    n = random.randint(1, max_length)\n",
    "    matrix = qt.gates.qeye(2) #identity matrix\n",
    "    for i in range(n) :\n",
    "         mat = np.random.choice(lst)\n",
    "         matrix *= mat\n",
    "    return matrix\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\n",
    "    \"cuda\" if torch.cuda.is_available() else\n",
    "    \"mps\" if torch.backends.mps.is_available() else\n",
    "    \"cpu\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a function that takes a quantum state and return the coordinates of the corresponding vector on the Bloch sphere."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def state_bloch(state) :\n",
    "    a = qt.basis(2, 0).dag() * state\n",
    "    b = qt.basis(2, 1).dag() * state\n",
    "    x = (2*a*b.conjugate()).real\n",
    "    y = (2*a*b.conjugate()).imag\n",
    "    z = abs(a)**2 - abs(b)**2\n",
    "    return np.array([x, y, z])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now build our gym environment, which is the environment the agent will interact with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuantumEnv(gym.Env) :\n",
    "    def __init__(self, gateSet, target_gate = sigmax()) :\n",
    "        self.state = qt.basis(2, 0)\n",
    "        self.bloch = state_bloch(self.state)\n",
    "       \n",
    "        self.targetState = target_gate * self.state\n",
    "        self.targetBloch = state_bloch(self.targetState)\n",
    "\n",
    "        self.bloch_sphere = qt.Bloch()\n",
    "        self.bloch_sphere.add_vectors(self.targetBloch)\n",
    "        self.bloch_sphere.add_points(self.bloch)\n",
    "\n",
    "        self.gateSet = gateSet\n",
    "        self.episode = 0\n",
    "        \n",
    "        self.action_space = spaces.Discrete(len(gateSet))\n",
    "        self.observation_space = spaces.Dict({\n",
    "            \"agent\" : spaces.Box(low = -1.0, high = 1.0, shape = (3,), dtype = np.float64),\n",
    "            \"target\" : spaces.Box(low = -1.0, high = 1.0, shape = (3,), dtype = np.float64)})\n",
    "\n",
    "\n",
    "    def get_obs(self) :\n",
    "        return {\n",
    "            \"agent\" : self.bloch,\n",
    "            \"target\" : self.targetBloch}\n",
    "    \n",
    "    def get_info(self) :\n",
    "        return {\n",
    "            \"fidelity\" : abs(self.state.dag() * self.targetState)**2\n",
    "        }\n",
    "    \n",
    "\n",
    "    def reset(self, seed: Optional[int] = None, options: Optional[dict] = None) :\n",
    "        super().reset(seed=seed)\n",
    "\n",
    "        self.state = qt.basis(2, 0)\n",
    "        self.bloch = state_bloch(self.state)\n",
    "       \n",
    "        self.targetState =  shuffling(gateList, 60) * self.state\n",
    "        self.targetBloch = state_bloch(self.targetState)\n",
    "\n",
    "        self.bloch_sphere = qt.Bloch()\n",
    "        self.bloch_sphere.add_vectors(self.targetBloch)\n",
    "        self.bloch_sphere.add_points(self.bloch)\n",
    "\n",
    "        observation = self.get_obs()\n",
    "        info = self.get_info()\n",
    "\n",
    "        self.episode = 0\n",
    "\n",
    "        return observation, info\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "    def step(self, action) :\n",
    "        self.state = self.gateSet[action] * self.state\n",
    "        self.bloch =  state_bloch(self.state)\n",
    "        self.bloch_sphere.add_points(self.bloch)\n",
    "        terminated = False\n",
    "        truncated = False\n",
    "\n",
    "        self.episode = self.episode + 1\n",
    "        observation = self.get_obs()\n",
    "        info = self.get_info()\n",
    "        fidelity = info[\"fidelity\"]\n",
    "        if fidelity > 0.99 : #we aim for 99% fidelity\n",
    "            terminated = True\n",
    "        reward = 0 if terminated else -1\n",
    "        \n",
    "        if self.episode > 500 :\n",
    "            truncated = True\n",
    "    \n",
    "        \n",
    "        return observation, reward, terminated, truncated, info\n",
    "\n",
    "\n",
    "    def render(self, mode =\"human\") :\n",
    "         self.bloch_sphere.show()\n",
    "\n",
    "gym.register(\n",
    "    id = \"gymnasium_env/QuantumEnv\",\n",
    "    entry_point = QuantumEnv,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Replay Memory<h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use experience replay memory, which consists of storing the transitions done by the agent, then from it sampling randomly steps taken by the agent to have a batch of uncorrelated steps for the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building the neural network :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepQNetwork(nn.Module) :\n",
    "\n",
    "    def __init__(self, n_obs, n_actions ) :\n",
    "        super(DeepQNetwork, self).__init__()\n",
    "        self.layer1 = nn.Linear(n_obs, 128)\n",
    "        self.layer2 = nn.Linear(128, 128)\n",
    "        self.layer3 = nn.Linear(128, n_actions)\n",
    "\n",
    "    def forward(self, state):\n",
    "        state = F.relu(self.layer1(state))\n",
    "        state = F.relu(self.layer2(state))\n",
    "        return self.layer3(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make(\"gymnasium_env/QuantumEnv\",\n",
    "                gateSet = efficient_set)\n",
    "\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "obs, info = env.reset()\n",
    "\n",
    "n_agent = len(obs[\"agent\"])\n",
    "#policy network :\n",
    "policy = DeepQNetwork(n_agent + 1, n_actions).to(device)\n",
    "#target network\n",
    "target = DeepQNetwork(n_agent + 1, n_actions).to(device)\n",
    "target.load_state_dict(policy.state_dict())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\epsilon$-Greedy Algorithm : We use this algorithm to decide which action to pursue, compromising between exploration and exploitation : with probability $\\epsilon$, the algorithm explores meaning it takes new actions (random action chosen) and explores the potential reward, with probability $1 - \\epsilon$, the algorithm exploits the action which yields the highest reward (with the currently available information)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "gamma = 0.95\n",
    "start_epsilon = 0.85\n",
    "end_epsilon= 0.05\n",
    "decay = 1000\n",
    "tau = 0.005\n",
    "LearningRate = 1e-4\n",
    "\n",
    "\n",
    "\n",
    "def action_choice(state) :\n",
    "    threshhold = end_epsilon + (start_epsilon - end_epsilon)* \\\n",
    "    np.exp(- env.episode/decay)  ###-1 *\n",
    "    \n",
    "    if np.random < threshhold :\n",
    "        return torch.tensor(env.action_space.sample(), device=device, dtype=torch.long)\n",
    "    else :\n",
    "        with torch.no_grad() :\n",
    "            index = policy(state).max(1).indices.view(1,1)\n",
    "            return torch.tensor(env.action_space[index], device=device, dtype = torch.long)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> References :<h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- Aram W. Harrow, Benjamin Recht, Isaac L. Chuang, Efficient Discrete Approximations of Quantum Gates :\n",
    "\n",
    "https://arxiv.org/pdf/quant-ph/0111031\n",
    "\n",
    "- Lorenzo Moro, Matteo G. A. Paris, Marcello Restelli & Enrico Prati, Quantum compiling by deep reinforcement learning :\n",
    "\n",
    "https://www.nature.com/articles/s42005-021-00684-3 "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
