{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import math\n",
    "import random\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import qutip as qt\n",
    "from qutip import sigmax, sigmay, sigmaz\n",
    "from IPython import display\n",
    "from collections import deque\n",
    "from typing import Optional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$V_1 = \\frac{1}{\\sqrt{5}} \\begin{pmatrix}\n",
    "1 & 2i \\\\\n",
    "2i & 1\n",
    "\\end{pmatrix}, \\quad\n",
    "V_2 = \\frac{1}{\\sqrt{5}} \\begin{pmatrix}\n",
    "1 & 2 \\\\\n",
    "-2 & 1\n",
    "\\end{pmatrix}, \\quad\n",
    "V_3 = \\frac{1}{\\sqrt{5}} \\begin{pmatrix}\n",
    "1 + 2i & 0 \\\\\n",
    "0 & 1 - 2i\n",
    "\\end{pmatrix}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat1 = (1/np.sqrt(5)) * np.array([[1, 2j], [2j, 1]])\n",
    "mat2 = (1/np.sqrt(5)) * np.array([[1, 2], [-2, 1]])\n",
    "mat3 = (1/np.sqrt(5)) * np.array([[1 + 2j, 0], [0, 1 - 2j]])\n",
    "dim = [[2],[2]]\n",
    "efficient_set = []\n",
    "\n",
    "v1 = qt.Qobj(mat1, dims = dim)\n",
    "v2 = qt.Qobj(mat2, dims = dim)\n",
    "v3 = qt.Qobj(mat3, dims = dim)\n",
    "\n",
    "efficient_set = [v1, v2, v3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = qt.gates.hadamard_transform()\n",
    "gateList = [h, sigmax(), sigmay(), sigmaz(), v1, v2, v3]\n",
    "\n",
    "def shuffling(lst, max_length) :\n",
    "    n = random.randint(1, max_length)\n",
    "    matrix = qt.gates.qeye(2) #identity matrix\n",
    "    for i in range(n) :\n",
    "         mat = np.random.choice(lst)\n",
    "         matrix *= mat\n",
    "    return matrix\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\n",
    "    \"cuda\" if torch.cuda.is_available() else\n",
    "    \"mps\" if torch.backends.mps.is_available() else\n",
    "    \"cpu\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def state_bloch(state) :\n",
    "    a = qt.basis(2, 0).dag() * state\n",
    "    b = qt.basis(2, 1).dag() * state\n",
    "    x = (2*a*b.conjugate()).real\n",
    "    y = (2*a*b.conjugate()).imag\n",
    "    z = abs(a)**2 - abs(b)**2\n",
    "    return np.array([x, y, z])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuantumEnv(gym.Env) :\n",
    "    def __init__(self, gateSet, target_gate = sigmax()) :\n",
    "        self.state = qt.basis(2, 0)\n",
    "        self.bloch = state_bloch(self.state)\n",
    "       \n",
    "        self.targetState = target_gate * self.state\n",
    "        self.targetBloch = state_bloch(self.targetState)\n",
    "\n",
    "        self.bloch_sphere = qt.Bloch()\n",
    "        self.bloch_sphere.add_vectors(self.targetBloch)\n",
    "        self.bloch_sphere.add_points(self.bloch)\n",
    "\n",
    "        self.gateSet = gateSet\n",
    "        self.episode = 0\n",
    "        \n",
    "        self.action_space = spaces.Discrete(len(gateSet))\n",
    "        self.observation_space = spaces.Dict({\n",
    "            \"agent\" : spaces.Box(low = -1.0, high = 1.0, shape = (3,), dtype = np.float32),\n",
    "            \"target\" : spaces.Box(low = -1.0, high = 1.0, shape = (3,), dtype = np.float32)})\n",
    "\n",
    "\n",
    "    def get_obs(self) :\n",
    "        return {\n",
    "            \"agent\" : self.bloch,\n",
    "            \"target\" : self.targetBloch}\n",
    "    \n",
    "    def get_info(self) :\n",
    "        return {\n",
    "            \"fidelity\" : abs(self.state.dag() * self.targetState)**2\n",
    "        }\n",
    "    \n",
    "\n",
    "    def reset(self, seed: Optional[int] = None, options: Optional[dict] = None) :\n",
    "        super().reset(seed=seed)\n",
    "\n",
    "        self.state = qt.basis(2, 0)\n",
    "        self.bloch = state_bloch(self.state)\n",
    "       \n",
    "        self.targetState =  shuffling(gateList, 60) * self.state\n",
    "        self.targetBloch = state_bloch(self.targetState)\n",
    "\n",
    "        self.bloch_sphere = qt.Bloch()\n",
    "        self.bloch_sphere.add_vectors(self.targetBloch)\n",
    "        self.bloch_sphere.add_points(self.bloch)\n",
    "\n",
    "        observation = self.get_obs()\n",
    "        info = self.get_info()\n",
    "\n",
    "        self.episode = 0\n",
    "\n",
    "        return observation, info\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "    def step(self, action) :\n",
    "        self.state = self.gateSet[action] * self.state\n",
    "        self.bloch =  state_bloch(self.state)\n",
    "        self.bloch_sphere.add_points(self.bloch)\n",
    "        terminated = False\n",
    "        truncated = False\n",
    "\n",
    "        self.episode = self.episode + 1\n",
    "        observation = self.get_obs()\n",
    "        info = self.get_info()\n",
    "        fidelity = info[\"fidelity\"]\n",
    "        if fidelity > 0.99 : #we aim for 99% fidelity\n",
    "            terminated = True\n",
    "        reward = 0 if terminated else -1\n",
    "        \n",
    "        if self.episode > 500 :\n",
    "            truncated = True\n",
    "    \n",
    "        \n",
    "        return observation, reward, terminated, truncated, info\n",
    "\n",
    "\n",
    "    def render(self, mode =\"human\") :\n",
    "         self.bloch_sphere.show()\n",
    "\n",
    "gym.register(\n",
    "    id = \"gymnasium_env/QuantumEnv\",\n",
    "    entry_point = QuantumEnv,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Replay Memory<h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepQNetwork(nn.Module) :\n",
    "\n",
    "    def __init__(self, n_obs, n_actions ) :\n",
    "        super(DeepQNetwork, self).__init__()\n",
    "        self.layer1 = nn.Linear(n_obs, 128)\n",
    "        self.layer2 = nn.Linear(128, 128)\n",
    "        self.layer3 = nn.Linear(128, n_actions)\n",
    "\n",
    "    def forward(self, state):\n",
    "        state = F.relu(self.layer1(state))\n",
    "        state = F.relu(self.layer2(state))\n",
    "        return self.layer3(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python312\\Lib\\site-packages\\gymnasium\\utils\\passive_env_checker.py:134: UserWarning: \u001b[33mWARN: The obs returned by the `reset()` method was expecting numpy array dtype to be float32, actual type: complex128\u001b[0m\n",
      "  logger.warn(\n",
      "c:\\Python312\\Lib\\site-packages\\gymnasium\\utils\\passive_env_checker.py:158: UserWarning: \u001b[33mWARN: The obs returned by the `reset()` method is not within the observation space.\u001b[0m\n",
      "  logger.warn(f\"{pre} is not within the observation space.\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make(\"gymnasium_env/QuantumEnv\",\n",
    "                gateSet = efficient_set)\n",
    "\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "obs, info = env.reset()\n",
    "\n",
    "n_agent = len(obs[\"agent\"])\n",
    "#policy network :\n",
    "policy = DeepQNetwork(n_agent + 1, n_actions).to(device)\n",
    "#target network\n",
    "target = DeepQNetwork(n_agent + 1, n_actions).to(device)\n",
    "target.load_state_dict(policy.state_dict())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\epsilon$-Greedy Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 256\n",
    "GAMMA = 0.95\n",
    "start_epsilon = 0.85\n",
    "end_epsilon= 0.05\n",
    "decay = 1000\n",
    "TAU = 0.005\n",
    "LearningRate = 1e-4\n",
    "\n",
    "\n",
    "\n",
    "def action_choice(state) :\n",
    "    threshhold = end_epsilon + (start_epsilon - end_epsilon)* \\\n",
    "    np.exp(- env.episode/decay)  ###-1 *\n",
    "    \n",
    "    if np.random < threshhold :\n",
    "        return torch.tensor(env.action_space.sample(), device=device, dtype=torch.long)\n",
    "    else :\n",
    "        with torch.no_grad() :\n",
    "            index = policy(state).max(1).indices.view(1,1)\n",
    "            return torch.tensor(env.action_space[index], device=device, dtype = torch.long)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.AdamW(policy.parameters(), lr=LearningRate, amsgrad=True)\n",
    "memory = ReplayMemory(10000)\n",
    "\n",
    "def visualization() :\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
